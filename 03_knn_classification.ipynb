{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9c1a3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $k$-Nearest Neighbor\n",
    "### Foundations of Machine Learning\n",
    "### `github.com/ds4e/knn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d740cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "- Today we start with predictive analytics, which is a vast field that doubles in size every 30 days\n",
    "- I'm going to use `cars_env.csv`, but `land_mines.csv` and `diabetes.csv` are also great choices; for today, we need data with two numeric explanatory variables $(x_1, x_2)$, and a categorical outcome label, $y$, that takes at least two distinct values\n",
    "- There are a lot of new ideas to take in, since we're breaking a lot of new ground; we'll revisit everything and explore the hidden depth throughout the rest of the course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d23ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "- Let's start with an example of what we're talking about: Predicting vehicle class from price and carbon footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55d131",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('./data/cars_env.csv')\n",
    "df.head()\n",
    "\n",
    "# Drop extremely expensive cars:\n",
    "q90 = np.quantile( df['baseline price'],.9) # Compute the .9 quantile\n",
    "print(q90)\n",
    "keep = df['baseline price'] < q90  # Logical condition asserting price < .9 quantile\n",
    "df = df.loc[keep,:] # Use locator function to filter on a Boolean conditional\n",
    "df.describe()\n",
    "sns.kdeplot(data=df,x='baseline price')\n",
    "\n",
    "# Simplify the vehicle classification scheme:\n",
    "df['class'] = df['EPA class']\n",
    "df['class'] = df['class'].replace(['MIDSIZE CARS','COMPACT CARS','SUBCOMPACT CARS','TWO SEATERS','LARGE CARS'],'car')\n",
    "df['class'] = df['class'].replace(['SMALL STATION WAGONS','MIDSIZE STATION WAGONS'],'station wagon')\n",
    "df['class'] = df['class'].replace(['STANDARD PICKUP TRUCKS','SMALL PICKUP TRUCKS'],'truck')\n",
    "df['class'] = df['class'].replace(['VANS','MINIVAN'],'van')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd14a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(x=df['baseline price'], hue = df['class'], common_norm=False)\n",
    "df.loc[:,['baseline price','class']].groupby('class').describe() # Baseline price by simplified vehicle class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ca22b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(x=df['footprint'], hue = df['class'], common_norm=False)\n",
    "df.loc[:,['footprint','class']].groupby('class').describe() # Footprint by simplified vehicle class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a08ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot footprint against price\n",
    "this_plot = sns.scatterplot(data=df,x='footprint',y='baseline price',\n",
    "                            hue='class',\n",
    "                            style='class')\n",
    "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1)) # Move legend off the plot canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af660f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "- There is a target/outcome variable $y$, which we are trying to predict using features/covariates $X$\n",
    "- All of machine learning works on a fairly simple premise: \"If $X$ and $X'$ are similar, then $y$ and $y'$ are probably similar.\"\n",
    "- A **(point) predictive model** is the mapping from features/covariates $\\hat{x}$ to target/outcome $\\hat{y}$, so $\\hat{y} = m(\\hat{x})$\n",
    "- We often predict other things, like the distribution of $\\hat{y}$ conditional on $\\hat{x}$, which we'll talk about"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1f5e7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification versus Regression\n",
    "- If the target/outcome variable $y$ is...\n",
    "    - Categorical: Then we are doing classification\n",
    "    - Numeric: Then we are doing regression\n",
    "- Our focus today is on a supervised learning algorithm for classification, called **$k$ nearest neighbor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a709b51",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outline\n",
    "1. $k$-NN Classification\n",
    "2. Selecting $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8d7fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. $k$-NN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9463a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preliminaries: Distance\n",
    "- In one dimensional, distance means absolute value: $d(a,b) = |a-b| = \\sqrt{ (a-b)^2}$\n",
    "- In many dimensions, distances usually means **Euclidean distance**. Imagine we have lists of variables for two observations, $a = [a_1, ..., a_L]$ and $b  = [b_1, ..., b_L]$. Then the distance between $a$ and $b$ is defined as:\n",
    "$$\n",
    "d(a,b) = ||a-b|| = \\sqrt{ (a_1 - b_1)^2 + (a_2 - b_2)^2 + ... + (a_n - b_L)^2}\n",
    "$$\n",
    "- What's the distance between $(1,3)$ and $(4,-2)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc97ba7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preliminaries: Scaling\n",
    "- Since units are typically non-comparable across variables (e.g. age and height), we need to **normalize** or **scale** the control/feature variables before we chuck them into a model\n",
    "- This is a very common transformation step, and many people do it automatically\n",
    "- To **Min-Max Scale** a variable $x$, we compute, for each observation $i$,\n",
    "$$\n",
    "u_i = \\dfrac{x_i - \\min(x)}{\\max(x)-\\min(x)}\n",
    "$$\n",
    "- This transforms all the values of $x$ so that that they lie between 0 and 1, with the smallest value mapped to zero and the largest value mapped to 1\n",
    "- If you do not do this (or if the variables aren't already similarly scaled), neighbor methods simple won't work correctly\n",
    "- It's easier to transform the data and keep the original, untransformed version to compare with predictions later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07593fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preliminaries: The `df.apply(fcn)` Method\n",
    "- If we want to use `fcn` on every column of `df` without looping over it, we can use the `.apply(fcn)` method from Pandas\n",
    "- So we either `from sklearn.preprocessing import MinMaxScaler` or\n",
    "```python\n",
    "def MinMaxScaler(x):\n",
    "    u = (x-min(x))/(max(x)-min(x))\n",
    "    return u\n",
    "\n",
    "df = df.apply(MinMaxScaler)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ac51e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# $k$-NN Classification\n",
    "- Imagine we have a categorical variable $y$ of interest, that we want to predict based on features/covariates $X$: Disease based on patient symptoms, corporate or sovereign rating based on financial indicators, genus or species based on characteristics or DNA\n",
    "- This is a classical **classification** problem: Given variables $X$, what is the probability we would assign to each possible label $\\ell$ for the variable $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d8895",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $k$ Nearest Neighbor Classification\n",
    "- Imagine we have covariates/features $X = [x_1, x_2, ..., x_N]$, consisting of $N$ observations of $L$ variables, and observed outcomes/target variable $Y$\n",
    "- Consider a new case $\\hat{x} = (\\hat{x}_1,...,\\hat{x}_L)$. We want to make a guess of what **class/label** it will likely take, $\\hat{\\ell}$\n",
    "- The *$k$ Nearest Neighbor Classification Algorithm* is:\n",
    "  1. Compute the distance from $\\hat{x}$ to each observation $x_i$ in the dataset\n",
    "  2. Find the $k$ \"nearest neighbors\" $x_1^*$, $x_2^*$, ..., $x_k^*$ to $\\hat{x}$ in the data in terms of distance, with outcome labels $\\ell_{1}^*$, $\\ell_2^*$, ..., $\\ell_k^*$\n",
    "  3. Return either a\n",
    "        - **Hard Classification**: The modal/most common label among the nearest neighbor labels\n",
    "        - **Soft Classification**: The proportions for which each of the labels occur among the nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad4031c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Getting Started\n",
    "- We need a categorical outcome $Y$ and (for now) two numeric covariates/features $(X_1, X_2)$\n",
    "- Make a Seaborn plot of the labels as a function of the covariates/features and their labels (Hint: Use the `markers=Y` argument)\n",
    "- How would kNN roughly work, looking at this picture?\n",
    "- Check in with your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34db092",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SciKit-Learn\n",
    "- Unless we are doing something tailored to a particular task, we generally don't want to code our own algorithms: It is time consuming, and existing implementations are typically more efficient or robust than what we would create (but there is a lot of value in coding your own algorithms and estimators)\n",
    "- The most popular Python machine learning library is called **SciKit-Learn**\n",
    "- You typically import it as `from sklearn.<model class> import <model name>`, where `<model class>` is the set of related models and `<model name>` is the desired algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1454117",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scikit-Learn\n",
    "- To get the `from sklearn.neighbors import KNeighborsClassifier` for classification\n",
    "- The workflow with `sk` is that you use it to\n",
    "  1. Create an untrained model object with a fixed $k$: `model = KNeighborsRegressor(n_neighbors=k)` or `model = KNeighborsClassifier(n_neighbors=k)`\n",
    "  2. Fit that object to the data, $(X,y)$: `fitted_model = model.fit(X,y)`\n",
    "  3. Use the fitted object to make predictions for new cases $\\hat{x}$: `y_hat = fitted_model.predict(x_hat)` for hard classification and `y_hat = fitted_model.predict_proba(x_hat)` for soft classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36a7a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Preamble and Read data\n",
    "\n",
    "Here's a template for fitting a $k$-NN classifier:\n",
    "\n",
    "```python\n",
    "# Preamble:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def minmax(x):\n",
    "    u = (x-min(x))/(max(x)-min(x))\n",
    "    return u\n",
    "\n",
    "# Wrangle data:\n",
    "df = pd.read_csv(file.csv) # load your data\n",
    "df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2fb3e3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Select Data and Normalize\n",
    "\n",
    "```python\n",
    "# Select data:\n",
    "y = df['target'] # Set out outcome/target\n",
    "ctrl_list = [ var_1, var_2, ] # List of control variables\n",
    "x = df.loc[:, ctrl_list] # Set our covariates/features\n",
    "u = x.apply(MinMaxScaler) # Scale our variables\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13eaa8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3. Fit a kNN Classifier and Predict\n",
    "```python\n",
    "# Set the number of neighbors, typically odd to \"break ties\":\n",
    "k = 5 \n",
    "# Create a fitted model instance:\n",
    "model = KNeighborsClassifier(n_neighbors = k) # Create a model instance\n",
    "model = model.fit(u,y) # Fit the model\n",
    "# Make predictions:\n",
    "y_hat = model.predict(u) # Hard prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c86a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Visualizing the Predictor\n",
    "- Sometimes, it is nice to try to visualize how the algorithm works\n",
    "```python\n",
    "this_plot = sns.scatterplot(x=x1,y=x2,\n",
    "                hue = y,\n",
    "                style=y_hat)\n",
    "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21aec2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Confusion Matrices\n",
    "- Did it... work?\n",
    "- Evaluating fit and understanding whether we're making good or bad predictions is a core skill in machine learning\n",
    "- For classification tasks, our most basic metric of \"Did it work?\" is a confusion matrix: Cross-tabulate the true labels with the predicted ones, and see if they align or not \n",
    "\n",
    "```python\n",
    "## Confusion Matrix:\n",
    "pd.crosstab(y,y_hat)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y, y_hat)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423085ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Accuracy\n",
    "- In many scenarios, we want to simply the confusion matrix into a single, summary number\n",
    "- Many, many ways of doing this have been proposed\n",
    "- The simplest and most obvious is, \"What proportion of the cases did we predict correctly?\" This is called **accuracy**\n",
    "- We sum up the number of cases on the \"descending diagonal\" of the confusion matrix, and divide by the total number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9ebfe2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Accuracy\n",
    "- Numpy:\n",
    "```python\n",
    "np.sum( np.diag(pd.crosstab(y,y_hat)))/len(y)\n",
    "```\n",
    "- From scikit:\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y,y_hat)\n",
    "```\n",
    "or\n",
    "```python\n",
    "model.score(u,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc8c41",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "- Pick a dataset, select appropriate target variable and features, fit a $k$-NN classifier, and compute the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614d97c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Selecting $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3afde01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hyperparameters\n",
    "- The variable $k$ is our first **hyper-parameter**: There's not an obvious way to pick it, just from looking at the data and the model themselves\n",
    "- Our models are typically very **greedy**: If you let them decide, they tend to pick more variables, more complexity, more nuance\n",
    "- This leads to models that are overconfident and unreliable, when used in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f311a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_grid = np.array([ (2*k+1)**2 for k in range(0,11)]) # Odd number grid\n",
    "accuracies = []\n",
    "for k in k_grid: # For each candidate value of k...\n",
    "    model = KNeighborsClassifier(n_neighbors = k) # Create a model instance\n",
    "    model = model.fit(u,y) # Fit the model\n",
    "    y_hat = model.predict(u) # Hard prediction\n",
    "    correct = (y==y_hat).astype(int)\n",
    "    sns.scatterplot(x=df['baseline price'],y=df['footprint'],\n",
    "                    hue = correct,\n",
    "                    style= correct)\n",
    "    plt.show() # Render plot\n",
    "    acc = model.score(u,y) # Compute accuracy\n",
    "    print( f'Accuracy for {k} neighbors is {acc}')\n",
    "    accuracies.append(acc) # Save accuracy\n",
    "    \n",
    "sns.lineplot(x=k_grid,y=accuracies).set(title='Accuracy',xlabel='Neighbors')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b9e8a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Over-fitting on the Test Set\n",
    "- What this says is: \"Pick $k=1$! That means, when you compare the predicted label to the actual label for every instance in the training data, you get the right answer!\"\n",
    "- As we increase $k$, we start making mistakes by taking the majority class label in a neighborhood of that size\n",
    "- This logic is independent of the application. Is $k=1$ always the answer?\n",
    "- Everything we did made sense, but we're missing something"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1effd27",
   "metadata": {},
   "source": [
    "## Keywords: Overfitting, Underfitting\n",
    "- If we pick $k$ too low, the model is overly sensitive to a handful of data points; this is **overfitting**\n",
    "- If we pick $k$ too high, the model averages over many observations and will give answers close to population proportions; this is **underfitting**\n",
    "- We want to avoid under/over-fitting by building robust, appropriately complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27416775",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What are we doing wrong?\n",
    "- In machine learning, at this level, we are trying to **predict**\n",
    "- So our thought experiment is this: \"Tomorrow, a new case walks through the door and you use your model. Is it likely to be accurate, or not?\"\n",
    "- What we were doing is asking, \"On the **training data** which the model has already seen, does the model perform well or not?\"\n",
    "- We want to replicate the experience of making predictions with the model, on new cases which the model has not yet seen\n",
    "- How do we replicate this kind of uncertainty?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f683f827",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Train-Test Splits\n",
    "- A **train-test split** randomly partitions the data into two sets:\n",
    "    1. Training data, usually about 80% of the sample, on which the model is trained\n",
    "    2. Test data, usually about 20% of the sample, on which performance metrics like confusion matrices and accuracy are computed\n",
    "- We are imagining that the test set is *like* the future cases we have not yet observed (i.e. drawn from the same joint distribution, having the same data generating process), and model performance on the test set will be similar to future performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb364b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Train-Test Splits\n",
    "- A train test split with Scikit:\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "u_train, u_test, y_train, y_test = train_test_split(u,y, # Feature and target variables\n",
    "                                                    test_size=.2, # Split the sample 80 train/ 20 test\n",
    "                                                    random_state=100) # For replication purposes\n",
    "```\n",
    "- A train-test split of the dataframe with Pandas:\n",
    "```python\n",
    "df = df.sample(frac=1, random_state = 100)\n",
    "test_size = int( .2 * len(df) ) \n",
    "df_test = df.iloc[:test_size,:].reset_index(drop=True)\n",
    "df_train = df.iloc[test_size:,:].reset_index(drop=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa501d7d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "u_train, u_test, y_train, y_test = train_test_split(u,y, \n",
    "                                                    test_size=.2,\n",
    "                                                    random_state=100) \n",
    "# Set the number of neighbors, typically odd to \"break ties\":\n",
    "k = 5 \n",
    "# Create a fitted model instance:\n",
    "model = KNeighborsClassifier(n_neighbors = k) # Create a model instance\n",
    "model = model.fit(u_train,y_train) # Fit the model, training set\n",
    "y_hat = model.predict(u_test) # Predictions, test set\n",
    "print( pd.crosstab(y_test, y_hat)) # Confusion matrix\n",
    "model.score(u_test,y_test) # Test accuracy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29255f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Choosing $k$\n",
    "- Now we loop over $k$, evaluating the accuracy on the test set to see what values of $k$ are best when confronted with new data:\n",
    "\n",
    "```python\n",
    "k_grid = np.array([ (2*k+3) for k in range(0,150)]) # Odd number grid\n",
    "test_accuracies = []\n",
    "train_accuracies = []\n",
    "for k in k_grid: # For each candidate value of k...\n",
    "    model = KNeighborsClassifier(n_neighbors = k) # Create a model instance\n",
    "    model = model.fit(u_train,y_train) # Fit the model\n",
    "    y_hat = model.predict(u_test) # Predict values\n",
    "    test_acc = model.score(u_test,y_test) # Compute test accuracy\n",
    "    train_acc = model.score(u_train,y_train) # Compute trainin accuracy\n",
    "    print( f'Test accuracy for {k} neighbors is {test_acc}; train accuracy for {k} neighbors is {train_acc}')\n",
    "    test_accuracies.append(test_acc) # Save test results\n",
    "    train_accuracies.append(train_acc) # Save training results\n",
    "    \n",
    "sns.lineplot(x=k_grid,y=train_accuracies,label='Training Accuracy').set(xlabel='Neighbors',ylabel='Accuracy')\n",
    "sns.lineplot(x=k_grid,y=test_accuracies,label='Test Accuracy')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0824054",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## \"Optimal\" $k$\n",
    "- Test Accuracy seems to peak between 35 and 65; it depends on the train-test split, and the granularity of the $k$ grid\n",
    "\n",
    "```python\n",
    "is_optimal = test_accuracies == np.max(test_accuracies) # Maximizer Boolean\n",
    "optimal_indices = np.where( is_optimal ) # Indices that maximize accuracy\n",
    "k_optimal = k_grid[ optimal_indices ] # Values of k that maximize accuracy\n",
    "```\n",
    "\n",
    "- I got $k^* = 43$ with an accuracy of .747\n",
    "- Training Accuracy peaks at $k=1$ and then declines, like we expected\n",
    "- This train-test split approach is somewhat noisy, and we'll introduce other, more robust tools\n",
    "- But this concept --- test model performance on data it has not been trained on --- is a core idea from machine learning that we will use over and over and explore in greater depth as the course goes on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33e4f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise:\n",
    "- Determine a good $k$ for your data from earlier."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
