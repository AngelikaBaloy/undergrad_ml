{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79a3c2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $k$-NN Regression\n",
    "### Foundations of Machine Learning\n",
    "### `https://www.github.com/ds4e/knn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a0b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "- Last time we did $k$-nearest neighbor classification:\n",
    "    - Supervised learning: Use $X$ to predict $y$\n",
    "    - Classification: $y$ was categorical, a label\n",
    "    - Performance: Confusion Matrix, Accuracy\n",
    "    - Hyperparameter Selection: Train-test Split\n",
    "- We make a few key changes:\n",
    "    - Supervised learning: Use $X$ to predict $y$\n",
    "    - **Regression: $y$ will be numeric, a number**\n",
    "    - **Performance: Residuals, Mean Squared Error**\n",
    "    - Hyperparameter Selection: Train-test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6cc404",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Example\n",
    "2. Regression\n",
    "3. Residuals and MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ef369",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c872b22",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Predicting Sales\n",
    "- Last time, we predicted vehicle class from footprint and price\n",
    "- Let's predict sales from price and mpg: How do `baseline mpg` and `baseline price` seem to drive `baseline sales`? \n",
    "- We care about this for marketing purposes (if we change our price, how many more cars do we sell?) as well as policy purposes (if we tax high carbon vehicles, how many fewer are sold?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1954fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cleaning the Data\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('./data/cars_env.csv')\n",
    "df.head()\n",
    "\n",
    "# Drop extremely expensive cars:\n",
    "q90 = np.quantile( df['baseline price'],.9) # Compute the .9 quantile\n",
    "print(q90)\n",
    "keep = df['baseline price'] < q90  # Logical condition asserting price < .9 quantile\n",
    "df = df.loc[keep,:] # Use locator function to filter on a Boolean conditional\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc605231",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sales versus Price\n",
    "``` python\n",
    "sns.scatterplot(x=df['baseline price'], \n",
    "                y = df['baseline sales'],\n",
    "                palette='crest',\n",
    "                alpha=.3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617da03f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sales versus MPG\n",
    "``` python\n",
    "sns.scatterplot(x=df['baseline mpg'], \n",
    "                y = df['baseline sales'],\n",
    "                palette='crest',\n",
    "                alpha=.3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be627e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prediction\n",
    "- This is the picture we want to predict: the color/sales, explained by price and mpg:\n",
    "```python\n",
    "\n",
    "sns.scatterplot(x=df['baseline price'], \n",
    "                y = df['baseline mpg'],\n",
    "                hue = np.log(df['baseline sales']),\n",
    "                palette='crest',\n",
    "                alpha=.6)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a75692",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "- Pick a data set in your groups\n",
    "- Find a well-posed regression problem (two explanatory numeric variables, one numeric outcome variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9ecdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e0a4a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $k$-NN Regression\n",
    "- Imagine we have covariates/features $X = [x_1, x_2, ..., x_N]$, consisting of $N$ observations of $L$ variables, and observed outcomes/target numeric variable $Y$\n",
    "- Consider a new case $\\hat{x} = (\\hat{x}_1,...,\\hat{x}_L)$. We want to make a guess of what **numeric value** it will likely take, $\\hat{y}$\n",
    "- The *$k$ Nearest Neighbor Regression Algorithm* is:\n",
    "  1. Compute the distance from $\\hat{x}$ to each observation $x_i$ in the dataset\n",
    "  2. Find the $k$ \"nearest neighbors\" $x_1^*$, $x_2^*$, ..., $x_k^*$ to $\\hat{x}$ in the data in terms of distance, with values $y_{1}^*$, $y_2^*$, ..., $y_k^*$\n",
    "  3. Return the average of the neighbor values, \n",
    "  \n",
    "  $$\\hat{y}(\\hat{x}) = \\dfrac{y_1^* + y_2^* + ... + y_k^*}{k} = \\frac{1}{k} \\sum_{i=1}^k y_i^*$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa3d9a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scikit-Learn\n",
    "- To get the `from sklearn.neighbors import KNeighborsRegressor` for regression\n",
    "- The workflow with `sk` is that you use it to\n",
    "  1. Create an untrained model object with a fixed $k$: `model = KNeighborsRegressor(n_neighbors=k)` or `model = KNeighborsClassifier(n_neighbors=k)`\n",
    "  2. Fit that object to the data, $(X,y)$: `fitted_model = model.fit(X,y)`\n",
    "  3. Use the fitted object to make predictions for new cases $\\hat{x}$: `y_hat = fitted_model.predict(x_hat)` for hard classification and `y_hat = fitted_model.predict_proba(x_hat)` for soft classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8205c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Train/Test Split\n",
    "- Remember, we don't want to pick hyperparameters or evaluate performance on the data on which the model is trained!\n",
    "- We want to imagine how the model with perform in plausible situations which the model has not yet seen\n",
    "- In the code that follows, we'll implement the train/test split straightaway, even though we're not picking the $k$ hyperparameter yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704c757",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## (1) Preamble, Read Data\n",
    "\n",
    "``` python\n",
    "# Preamble:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def minmax(x):\n",
    "    u = (x-min(x))/(max(x)-min(x))\n",
    "    return u\n",
    "\n",
    "# Wrangle data:\n",
    "df = pd.read_csv(file.csv) # load your data\n",
    "df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e3bf14",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## (2) Normalize, Train-Test Split\n",
    "``` python\n",
    "y = df['target'] # Set out outcome/target\n",
    "ctrl_list = [ var_1, var_2, ] # List of control variables\n",
    "x = df.loc[:, ctrl_list] # Set our covariates/features\n",
    "u = x.apply(MinMaxScaler) # Scale our variables\n",
    "\n",
    "u_train, u_test, y_train, y_test = train_test_split(u,y,test_size=.2,random_state=100) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318ae87",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## (3) Fit a kNN Regressor and Predict\n",
    "``` python\n",
    "# Set the number of neighbors, typically odd to \"break ties\":\n",
    "k = 5 \n",
    "# Create a fitted model instance:\n",
    "model = KNeighborsRegressor(n_neighbors = k) # Create a model instance\n",
    "model = model.fit(u_train,y_train) # Fit the model\n",
    "# Make predictions:\n",
    "y_hat = model.predict(u_test) # Prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83368a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## (4) Visualize the Predictor\n",
    "- Plotting the predicted against actual values provides a general, visual diagnostic for fit:\n",
    "```python\n",
    "sns.scatterplot(x=y_test, y=y_hat)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f10ded",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "- For the scenario you picked out, run the $k$-NN regression.\n",
    "- Plot your test and predicted values. Do they line up on the diagonal, or are there significant errors or patterns?\n",
    "- Commit/push your results back to GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f786a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Residuals and MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1429f14",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Residuals\n",
    "- The distance between the true value and the predicted value is called the **residual**,\n",
    "$$\\underbrace{r_i}_{\\text{Residual, error}} = \\underbrace{y_i}_{\\text{True}} - \\underbrace{\\hat{y}(x_i)}_{\\text{predicted}}$$\n",
    "- This is how far from the true outcome its predicted value was, or the error\n",
    "- These are like a \"confusion matrix\" for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d8bc0a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = np.asarray(y)\n",
    "y_hat = np.asarray(y_hat)\n",
    "\n",
    "# Perfect prediction line\n",
    "lo = np.min([y.min(), y_hat.min()])\n",
    "hi = np.max([y.max(), y_hat.max()])\n",
    "plt.plot([lo, hi], [lo, hi], linestyle='--', label='y = ŷ')\n",
    "\n",
    "# Data points\n",
    "plt.scatter(y_hat, y, label='(ŷ, y)')\n",
    "\n",
    "# Residuals: vertical lines y_i - y_hat_i\n",
    "for a, b in zip(y_hat, y):\n",
    "    plt.vlines(a, a, b, color='red', linewidth=.5)\n",
    "\n",
    "plt.xlabel(\"Predicted ŷ\")\n",
    "plt.ylabel(\"Actual y\")\n",
    "plt.title(\"Residuals (r = y − ŷ)\")\n",
    "plt.legend()\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4785e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss Function: Mean Squared Error\n",
    "- Like accuracy for classification, we seek some kind of nice summary number for regression. How well did we do?\n",
    "- The most fundamental common metric for this is **mean squared error**:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2,\n",
    "$$\n",
    "or **root mean squared error**, \n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}.\n",
    "$$\n",
    "- This is essentially the distance from the true values to the predicted ones, weighted by sample size (as $n$ gets large, these values typically approach some fixed value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301f0e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mean Squared Error\n",
    "- The `model.score(u_test, y_test)` value is **not** MSE or RMSE, but instead something called $R^2$ that we'll cover later\n",
    "- To compute MSE or RMSE, you can write your own function:\n",
    "``` python\n",
    "def mse(y_test,y_hat):\n",
    "    mse = np.sum( (y_test - y_hat) ** 2 )/len(y_test)\n",
    "    return mse\n",
    "```\n",
    "or `from sklearn.metrics import mean_squared_error`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f79cd57",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Picking $k$\n",
    "- Again, we can pick $k$ by plotting MSE for a range of values of $k$, and finding the one with the lowest mean squared error:\n",
    "``` python\n",
    "k_grid = [ (2*k+1) for k in range(100) ] # Odd numbers from 1 to 201\n",
    "mses = [] # List to save MSEs\n",
    "for k in k_grid:\n",
    "    model = KNeighborsRegressor(n_neighbors = k) # Create a model instance\n",
    "    model = model.fit(u_train,y_train) # Fit the model\n",
    "    y_hat = model.predict(u_test) # Predict values\n",
    "    mses.append( mse(y_test, y_hat) ) # Compute and store MSE\n",
    "sns.lineplot(x=k_grid, y=mses).set(y_label='MSE', x_label='Neighbors') # Plot MSE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07d37f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Picking $k$\n",
    "- Since we're looking at the **minimum** of our error function, we want to find the lowest value that MSE takes over all $k$:\n",
    "``` python\n",
    "index_star = np.argmin( mses ) # Find minimizing index of mses\n",
    "k_star = k_grid[index_star] # Find value of k at that index\n",
    "```\n",
    "- This gives us the optimal number of neighbors, based on a single train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336fd2f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training vs Test MSE\n",
    "- Just to see the difference:\n",
    "```python\n",
    "k_grid = [ (2*k+1) for k in range(100)]\n",
    "mses = []\n",
    "mses_train = []\n",
    "for k in k_grid:\n",
    "    model = KNeighborsRegressor(n_neighbors = k) \n",
    "    model = model.fit(u_train,y_train) \n",
    "    y_hat = model.predict(u_test) \n",
    "    mses_train.append( mse(y_train, model.predict(u_train))) # Save training MSE\n",
    "    mses.append( mse(y_test, y_hat) ) # Save test MSE\n",
    "sns.lineplot(x=k_grid, y=mses, label = 'test MSE')\n",
    "sns.lineplot(x=k_grid, y=mses_train, label = 'training MSE')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec0afae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "- For the scenario you picked out, determine the optimal number of neighbors using a train/test split.\n",
    "- How does the MSE behave as you increase $k$? Is there a clear minimum?\n",
    "- Commit/push your results back to GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c3ee7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "- We now have a complete data science loop:\n",
    "    1. Wrangle data\n",
    "    2. EDA and Visualization to explore relationships\n",
    "    3. $k$-NN regression to predict numeric response variables, $k$-NN classification to predict categorical response variables\n",
    "    4. Train/Test split for hyperparameter selection\n",
    "- This is a good time to do a lab, and connect the loop from beginning to end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
